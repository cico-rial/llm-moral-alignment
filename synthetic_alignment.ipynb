{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecad9b4c",
   "metadata": {},
   "source": [
    "### Import our useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf79b0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/github/synthetic-generation-alignment/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "GRPO libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries for GRPO\n",
    "import unsloth\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import TrainingArguments,GenerationConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel, FastModel\n",
    "\n",
    "import torch\n",
    "\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "print(\"GRPO libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341403a7",
   "metadata": {},
   "source": [
    "### Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c5f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "\n",
    "def load_secrets(path=\"secrets.yaml\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734530f",
   "metadata": {},
   "source": [
    "### Let's define some model that we want to test!\n",
    "Keep an eye on newer models that might be interesting to try! hehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507f8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those can be specified if we use the transformers library\n",
    "\n",
    "model_names_transformers = {\n",
    "    \"gemma-2\": \"google/gemma-2-2b-it\",\n",
    "    \"gemma-3-270m\": \"google/gemma-3-270m-it\",\n",
    "    \"gemma-3-1B\": \"google/gemma-3-1b-it\",\n",
    "    \"qwen\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"falcon\": \"tiiuae/Falcon-H1-1B-Base\", # mmh dunno if you should try it!\n",
    "}\n",
    "\n",
    "model_names_unsloth = {\n",
    "    \"gemma-2\": \"unsloth/gemma-2-2b-it\",\n",
    "    \"gemma-3-270m\": \"unsloth/gemma-3-270m-it\",\n",
    "    \"gemma-3-1B\": \"unsloth/gemma-3-1b-it\",\n",
    "    \"qwen\": \"unsloth/Qwen3-0.6B\",\n",
    "    \"falcon\": \"unsloth/Falcon-H1-0.5B-Instruct\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdfd9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your token has been correctly loaded!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    secrets = load_secrets()\n",
    "    hf_token = secrets[\"HF_TOKEN\"]\n",
    "    print(\"Your token has been correctly loaded!\")\n",
    "\n",
    "except [KeyError, FileNotFoundError]:\n",
    "    print(\"Error: Have you created a secrets.yaml file yet??\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196df97a",
   "metadata": {},
   "source": [
    "## Let's use unsloth library to load our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c035f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 2048\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc9d95",
   "metadata": {},
   "source": [
    "We load it the model with quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77395a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Gemma3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.591 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = model_names_unsloth[\"gemma-3-270m\"],\n",
    "    max_seq_length = MAX_SEQ_LENGTH, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = True, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534be3e",
   "metadata": {},
   "source": [
    "We prepare the model for LoRa fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8a602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d45816d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6261664",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\" :  \"user\",   # <--- user query\n",
    "        \"content\" : \"You are playing a game against another user A. You must choose either action1 or action2. Depending on your action and A's action, \\\\\"\n",
    "        \"you each get a certain number of points. The goal is to score the highest number of points. The points are awarded as follows: \\n \\\\\"\n",
    "        \"If you play action1 and A plays action1, you get 3 and A gets 3. \\\\\"\n",
    "        \"If you play action1 and A plays action2, you get 0 and A gets 5. \\\\\"\n",
    "        \"If you play action2 and A plays action1, you get 5 and A gets 0. \\\\\"\n",
    "        \"If you play action2 and A plays action2, you get 1 and A gets 1. \\n \\\\\"\n",
    "        \"You have played with this opponent before. Last time, you played action1 and A played action2, so you got 0 points and A got 5 points. \"\n",
    "        \"What action would you take in order to achieve the highest possible score in points? \\\\\"\n",
    "        \"Your answer must follow this format exactly: choose either action1 or action2. Do not explain your reasoning. \\\\\"\n",
    "        \"Your answer:\"\n",
    "        # \"content\" : prompt\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "22711684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUTPUT: Choose action1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages,          # or this sequence if you want inputs to have the attention mask\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True\n",
    "                                       )\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "input_ids, attention_mask = inputs.input_ids, inputs.attention_mask\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=256,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "output = tokenizer.decode(output_ids[0][input_ids.shape[-1]:],\n",
    "                          skip_special_tokens=True)\n",
    "\n",
    "# print(\"\\nPROMPT:\", prompt)\n",
    "print(\"\\nOUTPUT:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4be7d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "MAX_PROMPT_LENGTH = 256\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_torch_fused\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = MAX_PROMPT_LENGTH,\n",
    "    max_completion_length = MAX_SEQ_LENGTH - MAX_PROMPT_LENGTH,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 50,\n",
    "    save_steps = 50,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1faf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    reward_funcs=lambda x:x,\n",
    "    processing_class = tokenizer,\n",
    "    args = training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1a5092c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(trainer):\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(messages,          # or this sequence if you want inputs to have the attention mask\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True\n",
    "                                       )\n",
    "    \n",
    "    raw_inputs = inputs\n",
    "\n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    input_ids, attention_mask = inputs.input_ids, inputs.attention_mask\n",
    "\n",
    "    output_ids = trainer.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    output = tokenizer.decode(output_ids[0][input_ids.shape[-1]:],\n",
    "                            skip_special_tokens=True)\n",
    "    \n",
    "    return raw_inputs, output\n",
    "\n",
    "def get_reward(output):\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "14358a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/github/synthetic-generation-alignment/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UnslothGRPOTrainer' object has no attribute 'step'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m     batch_reward\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     14\u001b[0m batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m : batch_prompt,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m : batch_output,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m : batch_reward\n\u001b[1;32m     18\u001b[0m }\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m(batch)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UnslothGRPOTrainer' object has no attribute 'step'"
     ]
    }
   ],
   "source": [
    "for episode in range(1):\n",
    "    batch_prompt = []\n",
    "    batch_output = []\n",
    "    batch_reward = []\n",
    "    for t in range(5):\n",
    "\n",
    "        prompt, output = get_output(trainer)\n",
    "        batch_prompt.append(prompt)\n",
    "        batch_output.append(output)\n",
    "\n",
    "        reward = get_reward(output)\n",
    "        batch_reward.append(reward)\n",
    "    \n",
    "    batch = {\n",
    "        \"prompt\" : batch_prompt,\n",
    "        \"completion\" : batch_output,\n",
    "        \"reward\" : batch_reward\n",
    "    }\n",
    "\n",
    "    trainer.step(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b831746",
   "metadata": {},
   "source": [
    "# Unlike PPO, GRPO doesn't have the step method implemented! this might be a problem for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6b872",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-moral-alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
